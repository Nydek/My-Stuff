Design Goals:

For this project our group decided to focus on creating a protocol that allowed for the maximum availability of nodes. We decided our protocol would have maximum availability if it allowed a client to immediately read and write to any node they could reach. Since we knew partitions made it impossible for us to avoid stale reads, we wanted our protocol to maintain consistency within partitions as best we could. Our strategy for creating consistency when fixing partitions is that the most recent change to a key value pair should be the one that wins out, and for this reason we time stamped changes in our hash table.

Overview of our protocol:
At birth every node is given a list ips for all other nodes in the network, is given a version number for their hash table which is initially set to 0 and increments any time the hash table changes, is assigned a unique integer rank, and sets themselves to be their own leader. Each node also has an association rank, which by default is their own rank, but changes as they connect to other nodes. On creation and at a short regular interval from then onward every node sends a heartbeat request all other nodes in the network for information, and to see if they are still connected. After every heartbeat request each node looks through all of the nodes they are received a response from to determine their leader is. This is done by comparing the rank, and rank by association of all nodes who responded to determine who has the highest authority. Whoever is ranked highest, becomes the current node's new leader and their ip, hash table version number, rank, and rank by association are all saved temporarily. If during a heartbeat the leader switches to a new leader, the current node sends their entire hash table to their new leader, who sends the same request to their leader, until the request reaches a node who is their own leader. That node uses the timestamps of the hash table to update any stale data they may have. After the merge request affects the uppermost leader they update their version number, and all nodes following that leader take notice of that on their next heartbeat and request the leader for an update. This causes the followers to update their version and continue the propagation of the merged hash tables throughout the network until it reaches all nodes in the concerned partition. This happens because if a node's leader stays the same, but the version number of the leaders hash table increases the current node requests their leader's entire hash table and uses it's timestamps to update their own.

writing(putting and deleting):
Putting and deleting are both handled in the same way, because if a value is deleted we still need a timestamp for the key to determine whether or not to propagate that delete to other nodes when partitions are healed. When a write is given to a node, that node first checks to see if they are their own leader. If they are not they pass the request to their leader and wait for a response. Eventually a node will get the request who is the leader and that node will change their hash table and send back a successful response. Since the node changed their hashtable their version number increments and all of their followers will take note of that on their next heartbeat. Followers will request updates and increment their own hash table versions in turn. This will continue until the change has completely propagated throughout the network.

Reads(getting):
Straightforward for the same reason availability is straightforward. A read is carried out entirely by the node that is accessed. This means that sometimes stale reads will occur, but availability is guaranteed unless no nodes are reachable.

Merging partitions:
Because the association rank of a node can be based on the association rank of another node, and all ranks are unique the highest ranked node in a partition will eventually be the be the leader by association of all other nodes. When merging partitions what this means is that the rank by association of the leader with the higher rank will spread into the other partition and after enough heartbeats all nodes in the new combined partition will agree on the same leader by association. Any nodes that previously weren't under the new leader will have already sent their hash tables through the network to the new leader and any new changes they had to contribute would propagate from the leader throughout the network.

How we achieve availability:
Our protocol achieves availability fairly easily, because even in the case that a node loses all contact with other nodes they will take over the role of leader themselves and become a partition of one. In this and all other cases our protocol would still allow clients to read from and write to any nodes, but there is always a risk that their read is stale and their write won't reach other clients.


Strengths of our protocol:
Our protocol is robust and uses a system of propagation and hash table merging to maintain the best possible consistency, while still being fully available. This occurs, because when partitions heal the nodes will recognize this thanks to heartbeats, and even if there are only two nodes connecting two large partitions the associated rank of one of the leaders will propagate through that connection to the entire other partition, so after some number of heartbeats everyone will agree on the same leader and all nodes will eventually have hash tables that agree with each other.

Weaknesses of our protocol:
We believe our protocol's biggest weakness that we see is its ability to run efficiently on a large scale.In fact we ran into some issues implementing a version of this protocol to complete all the unit tests, because early the quick requests from of the unit tests sometimes did not give the heartbeats enough time to propogate the data. Since all writes need to be funneled through the leader, and the more nodes in the system the more latency will be needed to propagate changes. The stress on the leader would be great whenever many writes occurred simultaneously, and that would affect the rate at which changes could propagate. This takes a lot away from our system, because our systems strength is its ability to maintain consistency within a partition. If our protocol is used in a situation where performance issues affect the reliability of reads, even for changes within a partition, then the protocol is considerably less useful.

How our protocol could be improved:
There are many complex situations that can happen which the implementation we did is not prepared for, but there are solutions to. Firstly, if a write occurs while a partition is being healed a nodes leader may change so that it forwards the write request to the same node that requested the write. With single threaded nodes this is a catastrophic failure, and can only be solved by having a timeout that errors out of the write. However, with multithreaded nodes this could actually be setup to send the write on back through the same nodes that originally requested the write. Secondly, on a large systems incredible number of nodes it may take many heartbeats for a change to propagate throughout the network. This is something that could also be solved by multithreading, because we could have the highest leader in the network spin up a thread to handle the fast and efficient propagation of specific changes throughout the network. And lastly during the steps in which hash tables are merged or updated timestamps could be used instead of hash table version numbers to minimize the amount of data that needs to be transferred and improve performance.

Testing results:
Our final implementation was able to consistently pass all 17 of the unit tests provided. Across about 10 runs we found the unit tests to complete in an average of 17.756 seconds. The fastest run we had on our completed version was 15.294 seconds and the slowest was 19.610 seconds. There was also an incident where we had a run take ~53 seconds, but still complete all 17 tests successfully. The delay seemed to have came from one of the unit tests stalling for an unreasonable amount of time, but we could not determine why.